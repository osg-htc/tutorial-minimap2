{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e65a75-97f1-4e07-af19-5ca78710ebe7",
   "metadata": {},
   "source": [
    "# Analyzing Many Data Files: Mapping Genomic Reads with Minimap2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969f3be9-db7b-45b7-8273-dea8a251f831",
   "metadata": {},
   "source": [
    "This tutorial will walk you through how to analyze many data files by submitting a workload of many jobs that can run in parallel. \n",
    "\n",
    "We are using a realistic genomics use cases, using [Minimap2](https://github.com/lh3/minimap2) to complete a long-read sequencing read mapping process. In the tutorial, you'll work with real data and see how high-throughput computing (HTC) can accelerate your genomics workflows or any workflow that involves analyzing many individual files or pieces of data\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "* Break down a large computational problem into many independent smaller tasks\n",
    "* Submit hundreds to thousands of jobs with a few simple commands\n",
    "* Use the Open Science Data Federation (OSDF) to manage file transfer during job submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a28996-34ca-4a35-9985-c6f6a7682986",
   "metadata": {},
   "source": [
    "## Getting Ready\n",
    "\n",
    "Before we begin, Let us make sure we are in our `tutorial-minimap2` directory by printing our working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f2e094-3d74-4b7a-a896-f2ac6fcc75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/tutorial-minimap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d6939-ccd7-4d2f-8a72-9722f903a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046c65d-e9b6-4cd2-ae51-74e046670bc7",
   "metadata": {},
   "source": [
    "We should see `/home/<username>/tutorial-minimap2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726e3e1-1bc5-434e-a2b4-29d7385fdca7",
   "metadata": {},
   "source": [
    "## Workload Components\n",
    "\n",
    "Before thinking about how to run a list of jobs, let's bring the components of our workload (data and software) onto this computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ee656-1211-406c-a9cc-335ea4d7fc99",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For the data, we will be using simulated Oxford Nanopore reads from the _Megaptera novaeangliae_ (humpback whale) genome. The reference genome was generated by [Carminati et al. (2024)](https://www.nature.com/articles/s41597-024-03922-9) and simulated reads were generated for this tutorial using [pbsim3](https://github.com/yukiteruono/pbsim3).\n",
    "<center><img src=\"notebook_images/whale_acs.webp\" width=\"500px\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d67db-cbdc-4791-95c5-740736694129",
   "metadata": {},
   "source": [
    "We have a script called `download_data.sh` that will download our bioinformatic data. Let's go ahead and run this script to download our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb497641-ba23-47d1-91f5-2988c1318454",
   "metadata": {},
   "outputs": [],
   "source": [
    "./download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5da90b-88a6-4ef9-97af-3af4726ca80d",
   "metadata": {},
   "source": [
    "Our sequencing data files, all ending in `.fastq` can now be found in a folder called `inputs`.\n",
    "\n",
    "Our data has been organized into folders for ease of use. The `inputs` folder contains our sequencing reads. Most of our individual files used to actually run the workflow will be found in the tutorial folder, organized like so: \n",
    "\n",
    "```\n",
    "    â”œâ”€â”€ tutorial-minimap2\n",
    "    â”‚   â”œâ”€â”€ inputs\n",
    "    â”‚   â”‚   â”‚   â”œâ”€â”€ humpback_whale_reads.fastq\n",
    "    â”‚   â”œâ”€â”€ outputs\n",
    "    â”‚   â”œâ”€â”€ logs\n",
    "    â”‚   â”‚   â”œâ”€â”€ log\n",
    "    â”‚   â”‚   â”œâ”€â”€ error\n",
    "    â”‚   â”‚   â”œâ”€â”€ output\n",
    "    â”‚   â”œâ”€â”€ software\n",
    "    â”‚   â”‚   â”œâ”€â”€ minimap2.def\n",
    "```\n",
    "\n",
    "A few files that will be used many times are placed in a separate location: \n",
    "   \n",
    "```\n",
    "    â”œâ”€â”€ /ospool/guest-ap/data/jovyan/\n",
    "    â”‚   â”œâ”€â”€ tutorial-minimap2\n",
    "    â”‚   â”‚   â”œâ”€â”€ inputs\n",
    "    â”‚   â”‚   â”‚   â”œâ”€â”€ humpback_whale_ref_genome.fasta.mmi\n",
    "    â”‚   â”‚   â”œâ”€â”€ software\n",
    "    â”‚   â”‚   â”‚   â”œâ”€â”€ minimap2_08OCT2025_v1.sif\n",
    "\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> While this is the directory structure we will be using for this tutorial, you can\n",
    "    organize your files in whatever way makes sense for you. Just be sure to update the paths in the\n",
    "    job submission file accordingly. It's important to have a clear organizational structure\n",
    "    when working with many files and jobs.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>OSPool Directory for Tutorial:</b> While you do have an OSPool <code>/ospool/guest-ap/data/jovyan/</code> directory, for this tutorial we will be using\n",
    "    the shared directory <code>/ospool/uc-shared/public/osg-training/</code> to host our data and software.\n",
    "    This is because the OSPool directory is not available to OSPool users in the guest-ap account. If you have your own OSPool account, you will have access to your own <code>ospool/ap##/<username>/data/</code> directory to use for your own data and software.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f3689-cc56-4c4d-a226-4a0f65a0bad7",
   "metadata": {},
   "source": [
    "### Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1974b-4ff1-489c-b590-bc5c15c476ee",
   "metadata": {},
   "source": [
    "The first step of any analysis is to get the software you need. On the OSPool, we recommend the use of an [Apptainer](https://apptainer.org/) containers to package software. Apptainer is a popular containerization technology in the scientific computing communities. It allows users to create and run containers that encapsulate software and its dependencies, ensuring consistency across different computing environments.\n",
    "\n",
    "In this tutorial, we will use a container that has uses Anaconda's miniconda3 to install Minimap2. The container was built using the definition file `minimap2.def` located in the `software` folder. This definition file contains all the instructions needed to build the container. This definition file includes [SAMTools](http://www.htslib.org/) and [BEDTools](https://bedtools.readthedocs.io/en/latest/) as well.\n",
    "\n",
    "If you already ran the `download_data.sh` script, your software environment has already been setup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d23304-daf6-4d46-a541-ad15e38af901",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls software/\n",
    "ls /ospool/guest-ap/data/jovyan/tutorial-minimap2/software/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844ab3dde9e0ccb",
   "metadata": {},
   "source": [
    "In the `software` directory you will find the container definition file `minimap2.def`. For time-saving sakes, our `download_data.sh` script downloaded a pre-built version of this container and placed it in `/ospool/guest-ap/data/jovyan/tutorial-minimap2/software/minimap2_08OCT2025_v1.sif`. This `.sif` file is the actual container image that we will use to run our jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbea2b9b849cffd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you wanted to replicate the container build, you could do so by using\n",
    "    the definition file and steps below:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4633ca-93a5-4f78-b674-d941c6123d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat software/minimap2.def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da55a4-9654-42ca-ba67-d05b834ebe65",
   "metadata": {},
   "source": [
    "And then running this command: \n",
    "\n",
    "```\n",
    "$ apptainer build minimap2_08OCT2025_v1.sif software/minimap2.def\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2e06fd53f935f",
   "metadata": {},
   "source": [
    "## HTCondor and its List of Jobs/Tasks to Run\n",
    "\n",
    "HTCondor is a **workload manager** that enables researchers to distribute computing tasks across many machinesâ€”an approach known as **High Throughput Computing (HTC)**. Unlike High Performance Computing (HPC), which focuses on tightly coupled parallel jobs, HTC excels at handling **many independent tasks**, each of which can run separately on different nodes. This makes it a perfect fit for bioinformatics workloads like read mapping, assembly, and sequence analysis, where datasets can be partitioned.\n",
    "\n",
    "When mapping reads, our FASTQ files often contain **tens of millions of reads**. Mapping all of these reads in a single job can take hours or days and often risks failure if a single node crashes. Instead, we can **split the FASTQ file** into smaller chunksâ€”say, 10,000 reads eachâ€”and map them *independently*. Each subset becomes its own **Condor job**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Sometimes you don't need to split data - it comes as many pieces already! Whatever the case, you want to think about the list of items that need to be processed, in order to analyze the items as many jobs. \n",
    "</div>\n",
    "\n",
    "HTCondor then:\n",
    "1. Queues these jobs in a **job list**.\n",
    "2. Sends them to available resources across the OSPool or your local cluster.\n",
    "3. Monitors for completion or failure.\n",
    "4. Collects all output files for recombination later.\n",
    "\n",
    "HTCondor uses this information to automatically execute hundreds or thousands of such tasks efficiently and fault-tolerantly, distributing your tasks across the entire OSPool.\n",
    "\n",
    "### Generating and Submitting a List of Jobs\n",
    "\n",
    "\n",
    "#### Example Conceptual Visualization\n",
    "\n",
    "```text\n",
    "HTCondor Queue:\n",
    " â”œâ”€â”€ job_001 â†’ maps subset_001.fastq\n",
    " â”œâ”€â”€ job_002 â†’ maps subset_002.fastq\n",
    " â”œâ”€â”€ job_003 â†’ maps subset_003.fastq\n",
    " â”œâ”€â”€ ...\n",
    " â””â”€â”€ job_100 â†’ maps subset_100.fastq\n",
    "```\n",
    "\n",
    "Each of these jobs performs the same mapping operation, just on a different chunk of reads across dozens of machines at once. Once all are complete, youâ€™ll have 100 smaller SAM/BAM files ready to merge into a single final alignment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce3fe4b599cb44",
   "metadata": {},
   "source": [
    "![HTCondor List of Jobs](notebook_images/listOfJobs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc69dd81ddb8f9",
   "metadata": {},
   "source": [
    "## Building Our List of Jobs\n",
    "\n",
    "To run the minimap2 mapping on one sample the command is:\n",
    "\n",
    "```\n",
    "minimap2 -ax map-ont <ref_genome> <reads_fastq> > <output_sam_file>\n",
    "```\n",
    "\n",
    "#### Splitting Our FASTQ Reads File\n",
    "\n",
    "We want to break down our single humpback_whale_reads.fastq file into many smaller files, each with 1,000 reads. We can use the `split` command to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f3fd4725bfe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "split -l 4000 inputs/humpback_whale_reads.fastq --additional-suffix=_humpback_whale_reads.fastq inputs/subset_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7307c2928af74b",
   "metadata": {},
   "source": [
    "This command splits the `humpback_whale_reads.fastq` file into smaller files, each containing 4000 lines (which corresponds to 1000 reads, since each read in a FASTQ file is represented by 4 lines). It prepends the prefix `subset_` to each split output file, this will help us in the next step when listing our input files for HTCondor. The output files will be named `subset_aa_humpback_whale_reads.fastq`, `subset_ab_humpback_whale_reads.fastq`, `subset_ac_humpback_whale_reads.fastq`, etc.\n",
    "\n",
    "Run the command below to see the files that were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1175ec50965151",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l inputs | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50a2d46c86d095",
   "metadata": {},
   "source": [
    "#### Creating a List of Jobs from Our Subset Files\n",
    "\n",
    "Now that we have our smaller FASTQ files, we can create a list of jobs to map each subset file to the reference genome using HTCondor. Each job will run the `minimap2` command on one of these subset files.\n",
    "\n",
    "The easiest ways to do this is by using the `ls` command to list all the subset files and saving the output into a text file version for this list of jobs. We can save this list to a file called `listOfReads.txt` on our project directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a677bfb887a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls inputs/subset_* | xargs -n 1 basename > listOfReads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5fa3a-c752-4891-a002-7dc8dddd92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "head listOfReads.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e53ba4f8012f56",
   "metadata": {},
   "source": [
    "## Submitting Our Jobs with HTCondor\n",
    "\n",
    "So we want to run the `minimap2` command for each of these samples. Our list of jobs will be based on the list of reads subset files -- we want to submit one job per sample. To do this, we need to make two things:\n",
    "\n",
    "- a list of our samples (we already have this in `sample_list.txt`) âœ…, and\n",
    "- a \"template\" for the jobs we want to run.\n",
    "\n",
    "Each of our jobs will run the same command, but with a different sample file. The job template will be the HTCondor submit file. Our job template will be a file called `minimap2.submit`. It will reference our executable script `run_minimap2.sh`. Now is a good time to open up both of these files and take a look at them.\n",
    "\n",
    "#### Examining the Job Executable File\n",
    "\n",
    "Lets start with the executable script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecedd79b04e4a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat run_minimap2.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325563a2f3a01c24",
   "metadata": {},
   "source": [
    "You may notice that the script `run_minimap2.sh` is a simple bash script that runs the `minimap2` command with three parameters: the reference genome, the input FASTQ file, and the output SAM file. The script uses positional parameters `$1`, `$2`, and `$3` to accept these inputs when the script is executed. This means that when you run the script, you need to provide these three arguments in the correct order. HTCondor will handle passing these parameters to the script when it submits each job using the `arguments` option in the submit file.\n",
    "\n",
    "The script also include a samtools sort command to convert the SAM output from minimap2 into a sorted BAM file, which is a more efficient format for storing aligned reads. The sorted BAM file is saved with the same base name as the output SAM file but with a `.bam` extension. Sorting your BAM files is a common practice in bioinformatics workflows, as many downstream analysis tools require sorted BAM files for optimal performance. This is especially important when merging multiple BAM files together (for example, the many BAM files you will generate mapping each of your read subsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe747ecabd1c1a56",
   "metadata": {},
   "source": [
    "#### Examining the Job Submit File\n",
    "\n",
    "The typical structure of an HTCondor submit file includes several key sections:\n",
    "\n",
    "##### Example of a Submit File\n",
    "\n",
    "```plaintext\n",
    "container_image = <path>                        # Path to the Apptainer container image (.sif file)\n",
    "\n",
    "executable = <path>                             # Path to the Apptainer container image (.sif file)\n",
    "arguments = <string>                            # Path to the Apptainer container image (.sif file)\n",
    "\n",
    "transfer_input_files = <list of paths>          # List of input files to be transferred to the execution site\n",
    "\n",
    "transfer_output_files = <list of paths>         # List of output files to be transferred back to the submit machine\n",
    "transfer_output_remaps = <key=value pair>       # Remap output file paths\n",
    "\n",
    "output = <path>                                 # Path to the standard output file\n",
    "error = <path>                                  # Path to the standard error file\n",
    "log = <path>                                    # Path to the log file\n",
    "\n",
    "request_cpus = <int>                            # Number of CPU cores to request\n",
    "request_disk = <int>                            # Amount of disk space to request (in KB)\n",
    "request_memory = <int>                          # Amount of memory to request (in MB)\n",
    "\n",
    "queue\n",
    "```\n",
    "\n",
    "The job template will be the HTCondor submit file. To start out, we're going to write a submit file that submits a list of one (samples), for testing. Let's examine the pre-written submit file to submit one of our minimap2 jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93cbaf3cf38fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat run_minimap2.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98799057b1a9b45",
   "metadata": {},
   "source": [
    "#### Understanding the HTCondor Submit File\n",
    "\n",
    "In HTCondor, we describe *what* we want to run and *what files and resources* the job needs in a **submit file**.\n",
    "This file acts as a set of instructions that HTCondor reads to queue and execute our job on the OSPool.\n",
    "\n",
    "Letâ€™s break down each part of the submit file below and explain what it does.\n",
    "\n",
    "##### ðŸ§© **Container and Executable Setup**\n",
    "\n",
    "```text\n",
    "container_image        = osdf:///ospool/uc-shared/public/osg-training/tutorial-minimap2/software/minimap2_08OCT2025_v1.sif\n",
    "executable             = run_minimap2.sh\n",
    "arguments              = humpback_whale_ref_genome.fasta.mmi subset_ab_humpback_whale_reads.fastq\n",
    "```\n",
    "\n",
    "- **`container_image`**\n",
    "  Specifies the Apptainer/Singularity image that contains the required software.\n",
    "  Using containers ensures reproducibility and that all dependencies (like `minimap2`) are available, regardless of which machine runs the job.\n",
    "\n",
    "- **`executable`**\n",
    "  The script or command that will be executed inside the container â€” in this case, our shell script `run_minimap2.sh`.\n",
    "\n",
    "- **`arguments`**\n",
    "  These are the command-line arguments passed to the executable. Here, weâ€™re giving it:\n",
    "  - the **reference genome index** (`humpback_whale_ref_genome.fasta.mmi`), and\n",
    "  - a **subset of reads** (`subset_ab_humpback_whale_reads.fastq`).\n",
    "\n",
    "##### ðŸ“¦ **Input and Output Files**\n",
    "\n",
    "```text\n",
    "transfer_input_files   = osdf:///ospool/uc-shared/public/osg-training/tutorial-minimap2/inputs/humpback_whale_ref_genome.fasta.mmi, inputs/subset_ab_humpback_whale_reads.fastq\n",
    "\n",
    "transfer_output_files  = mapped_subset_ab_mhumpback_whale_reads.fastq_reads_to_genome_sam_sorted.bam\n",
    "transfer_output_remaps = \"mapped_subset_ab_humpback_whale_reads.fastq_reads_to_genome_sam_sorted.bam = outputs/mapped_subset_ab_humpback_whale_reads.fastq_reads_to_genome_sam_sorted.bam\"\n",
    "```\n",
    "\n",
    "- **`transfer_input_files`**\n",
    "  Lists the files HTCondor should send along with the job to the remote machine.\n",
    "  These are copied automatically before the job starts.\n",
    "\n",
    "- **`transfer_output_files`**\n",
    "  Lists which files should be sent back when the job finishes.\n",
    "  If not listed, HTCondor assumes the outputs are not needed.\n",
    "\n",
    "- **`transfer_output_remaps`**\n",
    "  Renames or moves the returned output into a specific location in your working directory.\n",
    "  Here, the output BAM file is placed neatly inside an `outputs/` folder. If the folder doesnâ€™t exist, HTCondor will create it.\n",
    "  The syntax is `\"original_filename = new_path/filename\"`.\n",
    "  _This is especially useful for keeping your workspace organized when running many jobs._\n",
    "\n",
    "##### ðŸ§¾ **Logs and Diagnostics**\n",
    "\n",
    "```text\n",
    "output = ./logs/$(Cluster)_$(Process)_mapping_subset_ab_humpback_whale_reads.fastq_step2.out\n",
    "error  = ./logs/$(Cluster)_$(Process)_mapping_subset_ab_humpback_whale_reads.fastq_step2.err\n",
    "log    = ./logs/$(Cluster)_mapping_step2.log\n",
    "```\n",
    "\n",
    "- **`output`**\n",
    "  Captures anything the job prints to standard output (`stdout`), such as progress messages.\n",
    "\n",
    "- **`error`**\n",
    "  Captures any error messages (`stderr`) if something goes wrong.\n",
    "\n",
    "- **`log`**\n",
    "  A job-level event log that records job lifecycle information (when it started, ended, or failed).\n",
    "  The `$(Cluster)` and `$(Process)` macros are automatically filled in by HTCondor to make each log file unique.\n",
    "\n",
    "##### âš™ï¸ **Resource Requests**\n",
    "\n",
    "```text\n",
    "request_cpus   = 2\n",
    "request_disk   = 5 GB\n",
    "request_memory = 10 GB\n",
    "```\n",
    "\n",
    "These tell HTCondor **how much compute and storage** your job needs.\n",
    "HTCondor will only send your job to a machine that can meet these requirements.\n",
    "\n",
    "- `request_cpus`: number of CPU cores to allocate\n",
    "- `request_disk`: how much temporary disk space to provide\n",
    "- `request_memory`: how much RAM (system memory) to allocate\n",
    "\n",
    "If you're not sure where to start, a good rule of thumb is to start with 1-2 CPUs and 4-8 GB memory for typical bioinformatics tasks, then adjust based on your job's performance and needs.\n",
    "\n",
    "##### ðŸš€ **Queue Command**\n",
    "\n",
    "```text\n",
    "queue 1\n",
    "```\n",
    "\n",
    "Finally, `queue` tells HTCondor to **submit the job**.\n",
    "You can think of it like pressing the \"start\" button. Here we queue **1 job**, but in later examples you might queue hundreds at once!\n",
    "\n",
    "##### **Submitting the Job**\n",
    "When you submit it using:\n",
    "\n",
    "```bash\n",
    "condor_submit mapping_example.sub\n",
    "```\n",
    "\n",
    "HTCondor will handle **everything else** â€” transferring files, finding a machine to run the job, monitoring progress, and bringing your results back.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af128a5cdb06d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_submit run_minimap2.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b6e9f2bd2f91d",
   "metadata": {},
   "source": [
    "Check on your job status using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac24987e00dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf9809-97d3-414e-ad25-2af00752fb81",
   "metadata": {},
   "source": [
    "### Job Template â€” Preparing a Scalable Submit File\n",
    "\n",
    "So far, weâ€™ve seen how to create a submit file that runs **one job** â€” mapping a single FASTQ subset to a reference genome.\n",
    "But what if we have **dozens or hundreds of subsets** that we want to map *in parallel*?\n",
    "Instead of writing and submitting a separate file for each subset, HTCondor allows us to turn our single-job submit file into a **job template** that can automatically scale to many jobs.\n",
    "\n",
    "#### The Concept: From One Job â†’ Many Jobs\n",
    "\n",
    "HTCondorâ€™s strength lies in managing many independent tasks.\n",
    "To do this efficiently, we can make our submit file **scalable** â€” meaning it can accept **variables** that change for each queued job.\n",
    "This allows us to define one flexible template and use different input values for each job (for example, different FASTQ subsets).\n",
    "\n",
    "Letâ€™s start with our original single-job setup:\n",
    "\n",
    "```shell\n",
    "arguments = humpback_whale_ref_genome.fasta.mmi subset_ab_humpback_whale_reads.fastq\n",
    "```\n",
    "\n",
    "This command runs `run_minimap2.sh` on one subset. For example, `run_minimap2.sh humpback_whale_ref_genome.fasta.mmi subset_ab_humpback_whale_reads.fastq`.\n",
    "To scale up, we can replace the hard-coded subset name with a **variable**, such as `$(reads_subset)`.\n",
    "\n",
    "But what about the reference genome (`humpback_whale_ref_genome.fasta.mmi`)?\n",
    "In this case, we want to keep it constant across all jobs, so we leave it as is. Reference genomes are typically large files that donâ€™t change between jobs, so it doesn't make sense to make them into a variable.\n",
    "\n",
    "#### Using Variables\n",
    "\n",
    "HTCondor allows you to define **custom variables** that can be reused throughout your submit file.\n",
    "You can then assign different values to these variables for each job when queuing multiple tasks.\n",
    "\n",
    "For example:\n",
    "\n",
    "```shell\n",
    "arguments = humpback_whale_ref_genome.fasta.mmi $(reads_subset)\n",
    "transfer_input_files = osdf:///ospool//uc-shared/public/osg-training/tutorial-minimap2/inputs/humpback_whale_ref_genome.fasta.mmi, inputs/$(reads_subset)\n",
    "transfer_output_files = mapped_$(reads_subset)_reads_to_genome_sam_sorted.bam\n",
    "transfer_output_remaps = \"mapped_$(reads_subset)_reads_to_genome_sam_sorted.bam = outputs/mapped_$(reads_subset)_reads_to_genome_sam_sorted.bam\"\n",
    "output = ./logs/$(Cluster)_$(Process)_$(reads_subset)_mapping.out\n",
    "error  = ./logs/$(Cluster)_$(Process)_$(reads_subset)_mapping.err\n",
    "```\n",
    "\n",
    "Now, each time a job is queued, HTCondor will substitute the variable `$(reads_subset)` with the value you provide â€” just like a placeholder in a template.\n",
    "\n",
    "You may notice that we did not include the `$(reads_subset)` variable in the log file. This is because the log file is shared across all jobs in the queue, and including a job-specific variable would create multiple log files with the same name, leading to confusion. Instead, we use `$(Cluster)` and `$(Process)` to uniquely identify each job's output and error files.\n",
    "\n",
    "**Our recommendation is to keep a single log file per submit file submission**, which will contain entries for all jobs submitted from that file. Whereas the output and error files should be unique per job, which is why we include the `$(reads_subset)` variable in their filenames.\n",
    "\n",
    "#### Queuing Multiple Jobs with `queue` and Variable Lists\n",
    "\n",
    "You can now tell HTCondor to queue one job for each subset of data, assigning a different variable value each time.\n",
    "\n",
    "```text\n",
    "queue reads_subset from listOfReads.txt\n",
    "```\n",
    "\n",
    "The file `listOfReads.txt` might look like this:\n",
    "\n",
    "```\n",
    "subset_ab_humpback_whale_reads.fastq\n",
    "subset_cd_humpback_whale_reads.fastq\n",
    "subset_ef_humpback_whale_reads.fastq\n",
    "```\n",
    "\n",
    "When you run:\n",
    "\n",
    "```bash\n",
    "condor_submit run_multi_minimap2.sub\n",
    "```\n",
    "\n",
    "HTCondor will automatically create **many jobs**, substituting each `$(reads_subset)` in the template with one line from the file. This queuing strategy works similarly to a `while` or `for` loop in programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a49a0c6eec16c",
   "metadata": {},
   "source": [
    "### â›” Testing Before Fully Scaling Up â›”ï¸\n",
    "\n",
    "Before submitting hundreds of mapping jobs to the OSPool, itâ€™s best practice to **test your workflow with just a few jobs first**. This helps ensure your inputs, paths, and scripts are correct and that your jobs complete successfully within expected time and resource limits.\n",
    "\n",
    "The easiest way to test your submit file before scaling to the full dataset is by generating a smaller subset of your listOfReads.txt file using the `head` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5555b9d6dd68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "head -n 5 listOfReads.txt > testset_of_listOfReads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc25d7de9ee2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat testset_of_listOfReads.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a6c0adc14ade2",
   "metadata": {},
   "source": [
    "Use the `testset_of_listOfReads.txt` file in your submit file to test with just a few jobs. Edit your submit file to reference `testset_of_listOfReads.txt` instead of the full list of reads for the `queue` statement. Here is an example of what your test submit file might look like:\n",
    "\n",
    "```shell\n",
    "... Previous submit file content ...\n",
    "queue reads_subset from testset_of_listOfReads.txt\n",
    "```\n",
    "\n",
    "Once you have your test submit file ready, you can submit it to HTCondor using the `condor_submit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ed84699e63e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_submit run_multi_minimap2.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc6a7709525cfc0",
   "metadata": {},
   "source": [
    "Monitor the job queue using `condor_watch_q` to see the status of your jobs. This command provides a real-time view of your job's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d67d02783fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be64d5f-fb8b-4cb0-a232-d96dd150cb99",
   "metadata": {},
   "source": [
    "Notice that using a **single submit file**, we now have **multiple jobs in the queue**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ef450-bc02-49a4-9a64-fc65dc834f1d",
   "metadata": {},
   "source": [
    "It's always good practice to look at our standard error, standard out, and HTCondor log files to catch unexpected output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec99d85-a465-440f-a1c6-03cb502c6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b02cd2-3a55-4123-8086-afb2d582f984",
   "metadata": {},
   "source": [
    "### Scaling Up to a List of Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b56c58-5593-4c04-b51d-e2799915b22b",
   "metadata": {},
   "source": [
    "We can now combine our template and our list of samples to generate a list of jobs! See what our new submit file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07113b48f40559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat run_full_minimap2.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584dc02-9af9-4b6d-87fb-6cf08c764752",
   "metadata": {},
   "source": [
    "Two changes have turned our previous submit file into something that can submit many jobs at once:\n",
    "* We have incorporated our list, `listOfReads.txt` in the `queue` option at the end of the file. There are\n",
    "different ways to \"queue\" items from a list; we've chosen `queue <variable> from <list.txt>` as a good all-purpose option.\n",
    "* Wherever our job template has a value that will be unique for every job (the sample id), we have replaced\n",
    "the value from our first submit file with a variable, `$(reads_subset)`, which was defined in the queue statement.\n",
    "\n",
    "One way to think about this file is as an inverted for-loop for submitting jobs - where the for statement `queue reads_subset from sample_list.txt` is at the bottom of the file and the rest of the file above the for statement is the body of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9e9eb-8b31-4d2b-95a7-64df7e17567a",
   "metadata": {},
   "source": [
    "We're now ready to submit our list of jobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6392e-928f-4b8c-8aca-e8470a9a99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_submit run_full_minimap2.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff8d5f4-b7a3-4c1d-bb22-b1f9e26e04e8",
   "metadata": {},
   "source": [
    "We can check on the status of our multiple jobs in HTCondor's queue by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895390a-b20a-4076-90de-bcd94236c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "condor_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecf23c-1b3e-4539-b6cf-f78d80adc292",
   "metadata": {},
   "source": [
    "When ready, we can check our results in our `results/` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97431cb0-f605-415a-bd21-e9b45bf56479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66135640304c9636",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've completed the long-read minimap tutorial on the OSPool, you're ready to adapt these workflows for your own data and research questions. Here are some suggestions for what you can do next:\n",
    "\n",
    "ðŸ§¬ Apply the Workflow to Your Own Data\n",
    "* Replace the tutorial datasets with your own FASTQ files and reference genome.\n",
    "* Modify the mapping submit files to fit your data size, read type, and resource needs.\n",
    "\n",
    "ðŸ§° Customize or Extend the Workflow\n",
    "* Incorporate quality control steps (e.g., filtering or read statistics) using FastQC.\n",
    "* Use other mappers or variant callers, such as ngmlr, pbsv, or cuteSV.\n",
    "* Add downstream tools for annotation, comparison, or visualization (e.g., IGV, bedtools, SURVIVOR).\n",
    "\n",
    "ðŸ“¦ Create Your Own Containers\n",
    "* Extend the Apptainer containers used here with additional tools, reference data, or dependencies.\n",
    "* For help with this, see our [Containers Guide](https://portal.osg-htc.org/documentation/htc_workloads/using_software/containers/).\n",
    "\n",
    "ðŸš€ Run Larger Analyses\n",
    "* Submit thousands of mappings or alignment jobs across the OSPool.\n",
    "* Explore data staging best practices using the OSDF for large-scale genomics workflows.\n",
    "* Consider using workflow managers (e.g., [DAGman](https://portal.osg-htc.org/documentation/htc_workloads/automated_workflows/dagman-workflows/) or [Pegasus](https://portal.osg-htc.org/documentation/htc_workloads/automated_workflows/tutorial-pegasus/)) with HTCondor.\n",
    "\n",
    "ðŸ§‘â€ðŸ’» Get Help or Collaborate\n",
    "* Reach out to [support@osg-htc.org](mailto:support@osg-htc.org) for one-on-one help with scaling your research.\n",
    "* Attend office hours or training sessionsâ€”see the [OSPool Help Page](https://portal.osg-htc.org/documentation/support_and_training/support/getting-help-from-RCFs/) for details.\n",
    "\n",
    "### Software\n",
    "\n",
    "In this tutorial, we created a *starter* apptainer containers for Minimap2. This container can serve as a *jumping-off* for you if you need to install additional software for your workflows.\n",
    "\n",
    "Our recommendation for most users is to use \"Apptainer\" containers for deploying their software.\n",
    "For instructions on how to build an Apptainer container, see our guide [Using Apptainer/Singularity Containers](https://portal.osg-htc.org/documentation/htc_workloads/using_software/containers-singularity/).\n",
    "If you are familiar with Docker, or want to learn how to use Docker, see our guide [Using Docker Containers](https://portal.osg-htc.org/documentation/htc_workloads/using_software/containers-docker/).\n",
    "\n",
    "This information can also be found in our guide [Using Software on the Open Science Pool](https://portal.osg-htc.org/documentation/htc_workloads/using_software/software-overview/).\n",
    "\n",
    "### Data\n",
    "\n",
    "The ecosystem for moving data to, from, and within the HTC system can be complex, especially if trying to work with large data (> gigabytes).\n",
    "For guides on how data movement works on the HTC system, see our [Data Staging and Transfer to Jobs](https://portal.osg-htc.org/documentation/htc_workloads/managing_data/overview/) guides.\n",
    "\n",
    "### GPUs\n",
    "\n",
    "The OSPool has GPU nodes available for common use. If you would like to learn more about our GPU capacity, please visit our [GPU Guide on the OSPool Documentation Portal](https://portal.osg-htc.org/documentation/htc_workloads/specific_resource/gpu-jobs/).\n",
    "\n",
    "## Getting Help\n",
    "\n",
    "The OSPool Research Computing Facilitators are here to help researchers using the OSPool for their research. We provide a broad swath of research facilitation services, including:\n",
    "\n",
    "* **Web guides**: [OSPool Guides](https://portal.osg-htc.org/documentation/) - instructions and how-tos for using the OSPool and OSDF.\n",
    "* **Email support**: get help within 1-2 business days by emailing [support@osg-htc.org](mailto:support@osg-htc.org).\n",
    "* **Virtual office hours**: live discussions with facilitators - see the [Email, Office Hours, and 1-1 Meetings](https://portal.osg-htc.org/documentation/support_and_training/support/getting-help-from-RCFs/) page for current schedule.\n",
    "* **One-on-one meetings**: dedicated meetings to help new users, groups get started on the system; email [support@osg-htc.org](mailto:support@osg-htc.org) to request a meeting.\n",
    "\n",
    "This information, and more, is provided in our [Get Help](https://portal.osg-htc.org/documentation/support_and_training/support/getting-help-from-RCFs/) page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
